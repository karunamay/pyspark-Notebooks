{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe2fa891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kpathak/opt/anaconda3/envs/mamlTesting/lib/python3.8/site-packages/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93ee9b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://naomi-fridman.medium.com/install-pyspark-to-run-on-jupyter-notebook-on-windows-4ec2009de21f\n",
    "#https://github.com/spark-examples/pyspark-examples/\n",
    "#https://www.datacamp.com/tutorial/pyspark-tutorial-getting-started-with-pyspark\n",
    "#https://www.guru99.com/pyspark-tutorial.html\n",
    "#https://sparkbyexamples.com/pyspark/install-pyspark-in-anaconda-jupyter-notebook/\n",
    "#follow conda install for pyspark and findaprk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9beaa37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/kpathak/opt/anaconda3/envs/mamlTesting/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/10/30 19:09:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/10/30 19:09:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/30 19:09:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/10/30 19:09:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/10/30 19:09:16 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fd8418e97f0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc =  SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SparkByExamples.com\") \\\n",
    "    .getOrCreate()\n",
    "spark= sc\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed68e589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.101:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkByExamples.com</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd8418e97f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1347a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/30 19:10:26 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct: 6\n",
      "avg: 3400.0\n",
      "+------------------------------------------------------------+\n",
      "|collect_list(salary)                                        |\n",
      "+------------------------------------------------------------+\n",
      "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
      "+------------------------------------------------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|collect_set(salary)                 |\n",
      "+------------------------------------+\n",
      "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|8                                 |\n",
      "+----------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Count of Department &amp; Salary: 8\n",
      "count: Row(count(salary)=10)\n",
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|3000         |\n",
      "+-------------+\n",
      "\n",
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|4100        |\n",
      "+------------+\n",
      "\n",
      "+-------------------+\n",
      "|kurtosis(salary)   |\n",
      "+-------------------+\n",
      "|-0.6467803030303032|\n",
      "+-------------------+\n",
      "\n",
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|4600       |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|2000       |\n",
      "+-----------+\n",
      "\n",
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|3400.0     |\n",
      "+-----------+\n",
      "\n",
      "+--------------------+\n",
      "|skewness(salary)    |\n",
      "+--------------------+\n",
      "|-0.12041791181069571|\n",
      "+--------------------+\n",
      "\n",
      "+-------------------+-------------------+------------------+\n",
      "|stddev_samp(salary)|stddev_samp(salary)|stddev_pop(salary)|\n",
      "+-------------------+-------------------+------------------+\n",
      "|765.9416862050705  |765.9416862050705  |726.636084983398  |\n",
      "+-------------------+-------------------+------------------+\n",
      "\n",
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|34000      |\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:==============================================>       (172 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|20900               |\n",
      "+--------------------+\n",
      "\n",
      "+-----------------+-----------------+---------------+\n",
      "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
      "+-----------------+-----------------+---------------+\n",
      "|586666.6666666666|586666.6666666666|528000.0       |\n",
      "+-----------------+-----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Groupby\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "  \n",
    "  \n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "print(\"approx_count_distinct: \" + \\\n",
    "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
    "\n",
    "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
    "\n",
    "df.select(collect_list(\"salary\")).show(truncate=False)\n",
    "\n",
    "df.select(collect_set(\"salary\")).show(truncate=False)\n",
    "\n",
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show(truncate=False)\n",
    "print(\"Distinct Count of Department &amp; Salary: \"+str(df2.collect()[0][0]))\n",
    "\n",
    "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
    "df.select(first(\"salary\")).show(truncate=False)\n",
    "df.select(last(\"salary\")).show(truncate=False)\n",
    "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
    "df.select(max(\"salary\")).show(truncate=False)\n",
    "df.select(min(\"salary\")).show(truncate=False)\n",
    "df.select(mean(\"salary\")).show(truncate=False)\n",
    "df.select(skewness(\"salary\")).show(truncate=False)\n",
    "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
    "    stddev_pop(\"salary\")).show(truncate=False)\n",
    "df.select(sum(\"salary\")).show(truncate=False)\n",
    "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
    "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
    "  .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c293f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----------+-------+--------------+--------+------+\n",
      "|EmpID|EmpFname|EmpLname|Department|Project|       Address|     DOB|Gender|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+\n",
      "|    1|  Sanjay|   Mehra|        HR|     P1|Hyderabad(HYD)|01/12/76|     M|\n",
      "|    2|  Ananya|  Mishra|     Admin|     P2|    Delhi(DEL)|02/05/68|     F|\n",
      "|    3|   Rohan|   Diwan|   Account|     P3|   Mumbai(BOM)|01/01/80|     M|\n",
      "|    4|   Sonia|Kulkarni|        HR|     P1|Hyderabad(HYD)|02/05/92|     F|\n",
      "|    5|   Ankit|  Kapoor|     Admin|     P2|    Delhi(DEL)|03/07/94|     M|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read data # mysql\n",
    "#employee info table\n",
    "E1 = spark.read.option(\"header\",True).csv('/Users/kpathak/Library/CloudStorage/OneDrive-ManhattanAssociates/kpathak/ESSAY_COMP/Employee_info_table.csv')\n",
    "E1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "401b4fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+------+\n",
      "|EmpID|EmpPosition|DateOfJoining|Salary|\n",
      "+-----+-----------+-------------+------+\n",
      "|    1|    Manager|     01/05/22|500000|\n",
      "|    2|  Executive|     02/05/22| 75000|\n",
      "|    3|    Manager|     01/05/22| 90000|\n",
      "|    2|       Lead|     02/05/22| 85000|\n",
      "|    1|  Executive|     01/05/22|300000|\n",
      "+-----+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Employee position table\n",
    "#employee info table\n",
    "E2 = spark.read.option(\"header\",True).csv('/Users/kpathak/Library/CloudStorage/OneDrive-ManhattanAssociates/kpathak/ESSAY_COMP/Employee_Position.csv')\n",
    "E2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c75d940",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Temporary view 'e1' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#creating temporary view for the tables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mE1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43me1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mamlTesting/lib/python3.8/site-packages/pyspark/sql/dataframe.py:164\u001b[0m, in \u001b[0;36mDataFrame.createTempView\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateTempView\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates a local temporary view with this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    The lifetime of this temporary table is tied to the :class:`SparkSession`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateTempView\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mamlTesting/lib/python3.8/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mamlTesting/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Temporary view 'e1' already exists"
     ]
    }
   ],
   "source": [
    "#creating temporary view for the tables\n",
    "E1.createTempView(\"e1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "344faddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|EmpFname_U|\n",
      "+----------+\n",
      "|    SANJAY|\n",
      "|    ANANYA|\n",
      "|     ROHAN|\n",
      "|     SONIA|\n",
      "|     ANKIT|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q1. Write a query to fetch the EmpFname from the EmployeeInfo table in upper case and use the ALIAS name as EmpName.\n",
    "spark.sql(\"select UPPER(EmpFname) as EmpFname_U from E1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b2ee06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "E2.createTempView(\"e2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd21f148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q2. Write a query to fetch the number of employees working in the department ‘HR’.\n",
    "spark.sql(\"select count(*) from E1 where E1.Department='HR'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e6a71ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2022-10-30|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q3. Write a query to get the current date.\n",
    "# spark.sql(\"select SYSTDATE()\")\n",
    "spark.sql(\"select current_date()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "074cf9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|substr(EmpLname, 1, 4)|\n",
      "+----------------------+\n",
      "|                  Mehr|\n",
      "|                  Mish|\n",
      "|                  Diwa|\n",
      "|                  Kulk|\n",
      "|                  Kapo|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q4. Write a query to retrieve the first four characters of  EmpLname from the EmployeeInfo table.\n",
    "spark.sql(\"select substr(EmpLname,1,4) from E1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cab30c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|EmpID|\n",
      "+-----+\n",
      "|    2|\n",
      "|    3|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q7. Write q query to find all the employees whose salary is between 50000 to 100000.\n",
    "spark.sql(\"select EmpID from E2 where E2.Salary BETWEEN 50000 AND 100000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b3c77cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|EmpFname|\n",
      "+--------+\n",
      "|  Sanjay|\n",
      "|   Sonia|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q8. Write a query to find the names of employees that begin with ‘S’\n",
    "spark.sql(\"select EmpFname from E1 where E1.EmpFname like 'S%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17f24fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------------+------+\n",
      "|EmpID|EmpPosition|DateOfJoining|Salary|\n",
      "+-----+-----------+-------------+------+\n",
      "|    1|  Executive|     01/05/22|300000|\n",
      "|    1|    Manager|     01/05/22|500000|\n",
      "|    2|  Executive|     02/05/22| 75000|\n",
      "+-----+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q9. Write a query to fetch top N records.\n",
    "spark.sql(\"select * from E2 ORDER BY Salary LIMIT 3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "641c12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|concat(EmpFname,  , EmpLname)|\n",
      "+-----------------------------+\n",
      "|                 Sanjay Mehra|\n",
      "|                Ananya Mishra|\n",
      "|                  Rohan Diwan|\n",
      "|               Sonia Kulkarni|\n",
      "|                 Ankit Kapoor|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q10. Write a query to retrieve the EmpFname and EmpLname in a single column as “FullName”. The first name and the last name must be separated with space.\n",
    "spark.sql(\"select concat(EmpFname,' ',EmpLname) from E1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e54ed6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q11. Write a query find number of employees whose DOB is between 02/05/1970 to 31/12/1975 and are grouped according to gender\n",
    "\n",
    "spark.sql(\"select count(*) from E1 where E1.DOB Between '02/05/1970' AND '31/12/1975'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33e4c27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-----------+\n",
      "|EmpID|EmpFname|EmpPosition|\n",
      "+-----+--------+-----------+\n",
      "|    1|  Sanjay|    Manager|\n",
      "|    3|   Rohan|    Manager|\n",
      "+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q16. Write a query to fetch all employees who also hold the managerial position.\n",
    "spark.sql(\"select E1.EmpID,E1.EmpFname,E2.EmpPosition from E1,E2 where E1.EmpID==E2.EmpID AND E2.EmpPosition='Manager'\").show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "700981d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "|EmpID|EmpFname|EmpLname|Department|Project|       Address|     DOB|Gender|EmpID|EmpPosition|DateOfJoining|Salary|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "|    1|  Sanjay|   Mehra|        HR|     P1|Hyderabad(HYD)|01/12/76|     M|    1|    Manager|     01/05/22|500000|\n",
      "|    3|   Rohan|   Diwan|   Account|     P3|   Mumbai(BOM)|01/01/80|     M|    3|    Manager|     01/05/22| 90000|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from E1,E2 where E1.EmpID==E2.EmpID AND E2.EmpPosition='Manager'\").show( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1ba0f351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|Department|emp_count|\n",
      "+----------+---------+\n",
      "|     Admin|        2|\n",
      "|        HR|        2|\n",
      "|   Account|        1|\n",
      "+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 152:==============================================>      (177 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Q17. Write a query to fetch the department-wise count of employees sorted by department’s count in ascending order.\n",
    "spark.sql(\"select Department, count(EmpID) as emp_count from E1 group by Department order by emp_count desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "44d6b6fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`rowno`' given input columns: [e1.Address, e1.DOB, e1.Department, e1.EmpFname, e1.EmpID, e1.EmpLname, e1.Gender, e1.Project]; line 1 pos 26;\n'Project ['EmpID]\n+- 'Filter ('MOD('rowno, 2) = 0)\n   +- 'SubqueryAlias __auto_generated_subquery_name\n      +- 'Project ['rowno, EmpID#1545]\n         +- SubqueryAlias e1\n            +- Relation[EmpID#1545,EmpFname#1546,EmpLname#1547,Department#1548,Project#1549,Address#1550,DOB#1551,Gender#1552] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [85], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Q18. Write a query to calculate the even and odd records from a table.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mselect EmpID from (select rowno, EmpID from E1) where MOD(rowno,2)=0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect EmpID from (select rowno, EmpID from E1) where MOD(rowno,2)!=0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mamlTesting/lib/python3.8/site-packages/pyspark/sql/session.py:723\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msql\u001b[39m(\u001b[38;5;28mself\u001b[39m, sqlQuery):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a :class:`DataFrame` representing the result of the given query.\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;124;03m    [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mamlTesting/lib/python3.8/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mamlTesting/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`rowno`' given input columns: [e1.Address, e1.DOB, e1.Department, e1.EmpFname, e1.EmpID, e1.EmpLname, e1.Gender, e1.Project]; line 1 pos 26;\n'Project ['EmpID]\n+- 'Filter ('MOD('rowno, 2) = 0)\n   +- 'SubqueryAlias __auto_generated_subquery_name\n      +- 'Project ['rowno, EmpID#1545]\n         +- SubqueryAlias e1\n            +- Relation[EmpID#1545,EmpFname#1546,EmpLname#1547,Department#1548,Project#1549,Address#1550,DOB#1551,Gender#1552] csv\n"
     ]
    }
   ],
   "source": [
    "# Q18. Write a query to calculate the even and odd records from a table.\n",
    "spark.sql(\"select EmpID from (select rowno, EmpID from E1) where MOD(rowno,2)=0\").show()\n",
    "spark.sql(\"select EmpID from (select rowno, EmpID from E1) where MOD(rowno,2)!=0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb883cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "|EmpID|EmpFname|EmpLname|Department|Project|       Address|     DOB|Gender|EmpID|EmpPosition|DateOfJoining|Salary|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "|    1|  Sanjay|   Mehra|        HR|     P1|Hyderabad(HYD)|01/12/76|     M|    1|  Executive|     01/05/22|300000|\n",
      "|    1|  Sanjay|   Mehra|        HR|     P1|Hyderabad(HYD)|01/12/76|     M|    1|    Manager|     01/05/22|500000|\n",
      "|    2|  Ananya|  Mishra|     Admin|     P2|    Delhi(DEL)|02/05/68|     F|    2|       Lead|     02/05/22| 85000|\n",
      "|    2|  Ananya|  Mishra|     Admin|     P2|    Delhi(DEL)|02/05/68|     F|    2|  Executive|     02/05/22| 75000|\n",
      "|    3|   Rohan|   Diwan|   Account|     P3|   Mumbai(BOM)|01/01/80|     M|    3|    Manager|     01/05/22| 90000|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q19. Write a SQL query to retrieve employee details from EmployeeInfo table who have a date of joining in the EmployeePosition table.\n",
    "spark.sql(\"select * from E1, E2 where E1.EmpID=E2.EmpID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc8bf0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "|EmpID|EmpFname|EmpLname|Department|Project|       Address|     DOB|Gender|EmpID|EmpPosition|DateOfJoining|Salary|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "|    1|  Sanjay|   Mehra|        HR|     P1|Hyderabad(HYD)|01/12/76|     M|    1|  Executive|     01/05/22|300000|\n",
      "|    1|  Sanjay|   Mehra|        HR|     P1|Hyderabad(HYD)|01/12/76|     M|    1|    Manager|     01/05/22|500000|\n",
      "|    2|  Ananya|  Mishra|     Admin|     P2|    Delhi(DEL)|02/05/68|     F|    2|       Lead|     02/05/22| 85000|\n",
      "|    2|  Ananya|  Mishra|     Admin|     P2|    Delhi(DEL)|02/05/68|     F|    2|  Executive|     02/05/22| 75000|\n",
      "|    3|   Rohan|   Diwan|   Account|     P3|   Mumbai(BOM)|01/01/80|     M|    3|    Manager|     01/05/22| 90000|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+-----+-----------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from E1 inner join E2 where E1.EmpID=E2.EmpID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "245973a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "| 90000|\n",
      "| 85000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q20. Write a query to retrieve two minimum and maximum salaries from the EmployeePosition table.\n",
    "#two minimum slaraies\n",
    "spark.sql(\"select Salary from E2 order by Salary desc LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5c90076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "|300000|\n",
      "|500000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#two max slaraies\n",
    "spark.sql(\"select Salary from E2 order by Salary asc LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q21. Write a query to find the Nth highest salary from the table without using TOP/limit keyword. N=2\n",
    "spark.sql(\"select max(Salary) from (select * from E2 order by Salary desc) where rowno<=N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fc613485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Salary|\n",
      "+------+\n",
      "|500000|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/30 23:53:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Salary from (select Salary, dense_rank( ) over(order by Salary asc) r from E2) where r=2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216b8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select Salary from (Select Salary, dense_rank( ) over (order by Salary desc ) r from E2) where r=N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b45f3a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----------+---------+\n",
      "|EmpID|EmpFname|EmpLname|Department|row_count|\n",
      "+-----+--------+--------+----------+---------+\n",
      "|    3|   Rohan|   Diwan|   Account|        1|\n",
      "|    5|   Ankit|  Kapoor|     Admin|        1|\n",
      "|    1|  Sanjay|   Mehra|        HR|        1|\n",
      "|    4|   Sonia|Kulkarni|        HR|        1|\n",
      "|    2|  Ananya|  Mishra|     Admin|        1|\n",
      "+-----+--------+--------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q22. Write a query to retrieve duplicate records from a table.\n",
    "spark.sql(\"select EmpID, EmpFname, EmpLname, Department, count(*) as row_count from E1 group by EmpID, EmpFname, EmpLname, Department HAVING row_count >= 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1ad0c7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+----------+-------+--------------+--------+------+\n",
      "|EmpID|EmpFname|EmpLname|Department|Project|       Address|     DOB|Gender|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+\n",
      "|    1|  Sanjay|   Mehra|        HR|     P1|Hyderabad(HYD)|01/12/76|     M|\n",
      "|    2|  Ananya|  Mishra|     Admin|     P2|    Delhi(DEL)|02/05/68|     F|\n",
      "+-----+--------+--------+----------+-------+--------------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a query to fetch 50% records from the EmployeeInfo table.\n",
    "spark.sql(\"select * from E1 where EmpID <=(select count(distinct(EmpID))/2 from E1)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "be709763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
